---
title: AI Copilot
emoji: ü§ñ
colorFrom: blue
colorTo: indigo
sdk: gradio
sdk_version: 6.0.1
app_file: app/app.py
pinned: false
---

# AI Copilot ‚Äî MVP 

Autor: Marisol S. Herrera
Fecha: Diciembre 2025

AI Copilot es un MVP de asistente conversacional dise√±ado para demostrar integraci√≥n con un LLM abierto (Llama 4 Maverick v√≠a Groq), implementando l√≥gica conversacional controlada, seguridad, manejo de intents y recuperaci√≥n de contexto a corto plazo.
El sistema responde a necesidades de tareas diarias, b√∫squedas r√°pidas de informaci√≥n y apoyo educativo/productivo

## **1. Objetivo del Proyecto**
Construir un asistente conversacional con:

- Integraci√≥n estable con LLM (Groq).

- Control avanzado de timeouts, reintentos y fallbacks.

- Memoria conversacional corta y truncado de historial.

- Manejo de intents simples (/nota, /recordatorio, /busqueda, etc.)

- Guardrails de seguridad.
    
- M√©tricas de desempe√±o en latencia, uso de token y manejo de errores

## **2. Arquitectura del MVP**
    /core
        prompting.py        ‚Üí Plantillas system/user/assistant y truncado.
        conversation.py     ‚Üí Manejo del historial, intents y pipeline conversacional.
    
    /services
        llm.py              ‚Üí Cliente Groq (timeouts, retries, errores, m√©tricas).
    
    /app
        app.py              ‚Üí Interfaz Gradio para web demo.
    
    /tests
        test_prompting.py
        test_conversation.py
        test_llm.py
    
    .env.example            ‚Üí Variables de entorno (sin claves reales).
    README.md

## **3. Setup & Ejecuci√≥n Local**
**Instalar dependencias**
    pip install -r requirements.txt

**Variales de entorno**
Copiar .env.example ‚Üí .env y colocar:
    GROQ_API_KEY="TU_LLAVE_AQUI"
    MODEL_NAME="meta-llama/llama-4-maverick-17b-128e-instruct"

## **4. Integraci√≥n LLM**
**Modelo**
- meta-llama/llama-4-maverick-17b-128e-instruct

- Menor latencia, m√°s estable, econ√≥mico, perfecto para MVP.

**Par√°metros de inferencia**

- `temperature = 0.3` ‚Üí Respuestas estables y predecibles

- `top_p = 0.9` ‚Üí Variabilidad controlada

- `max_tokens` = 300 ‚Üí Limita costos

- `seed = 42` ‚Üí Reproducibilidad

**Control de fallos**
Implementado en `services/llm.py`:


| Error              | Acci√≥n                                |
| ------------------ | ------------------------------------- |
| 400                | No retry, fallback inmediato          |
| 401 / 403          | Fallback inmediato, mensaje claro     |
| 500 / 503          | Retry con backoff exponencial (max 2) |
| Timeout            | Tratado como 500 ‚Üí retry              |
| Exception gen√©rica | Fallback seguro                       |

## **5. L√≥gica de conversaci√≥n**
**Memoria**

Se conservan los √∫ltimos ~5 turnos (contexto corto).
L√≠mite de sesi√≥n: **20 turnos.**

**Intents soportados**

- `/nota` <texto>

- `/recordatorio` <texto>

- `/agenda`

- `/vernota` <texto>

- `/busqueda` <texto>

- Flujo por defecto si no coincide con un intent.

**Guardrails**

Antes de contactar al LLM:

- violencia extrema

- autoharm

- instrucciones ilegales (ej. bombas)

El sistema bloquea con:

    Lo siento, no puedo ayudar con esa solicitud.

## **6. Pruebas**

El proyecto se valid√≥ mediante pruebas unitarias, pruebas de integraci√≥n simulando errores del proveedor, y tres corridas E2E completas ejecutadas en entorno local.
A continuaci√≥n se documentan los resultados con precisi√≥n.

### **6.1. Pruebas Unitarias**

**Prompting**

‚úî System prompt siempre presente
‚úî Truncado correcto del historial
‚úî Inserci√≥n ordenada de mensajes (system ‚Üí history ‚Üí user)

**ConversationManager**

‚úî update_state mantiene solo √∫ltimos N turnos
‚úî pipeline detecta intents correctamente
‚úî reconoce /nota, /agenda, /busqueda, etc.

**LLM**

‚úî Test 400: retorna fallback sin retry
‚úî Test 500: retry, luego fallback
‚úî Test timeout: retry ‚Üí fallback
‚úî Test clave inv√°lida: error 401 ‚Üí fallback inmediato
‚úî Test success: devuelve contenido del modelo

***Resultado:**
**TODAS las pruebas unitarias pasan.**

### **6.2 Pruebas E2E**

**0. Prueba con API inv√°lida**
    GROQ_API_KEY="INVALID_TEST_KEY" python3 app/app.py

**Salida esperada y obtenida:**
- Log:

    INTENT | ... | Intent resolved: DEFAULT
    FALLBACK | ... | LLM service unavailable


- Mensaje al usuario:

    ‚ö†Ô∏è Modo fallback activado
    Hubo un problema al conectarme con el modelo...


- M√©tricas:

    total_calls: 1
    total_fallbacks: 1
    retries: 0
    latency: 0 ms


**Validaci√≥n de:**

- Manejo correcto de clave inv√°lida (401‚Äì403).

- Fallback visible + log en consola.

**1. Conversaci√≥n larga, 20 turnos**
Secuencia validada:

**Memoria de identidad**

- ‚ÄúMe llamo Marisol‚Äù ‚Üí guardada.

- ‚Äú¬øC√≥mo me llamo?‚Äù ‚Üí ‚ÄúTe llamas Marisol.‚Äù

**Creaci√≥n y recuperaci√≥n de notas**

- /nota Contrase√±a Ticketmaster sosososos ‚Ä¶

- Recuperada correctamente con /vernota Ticketmaster.

**Recordatorios con fecha**

/recordatorio Expo Ingenier√≠as‚Ä¶

Posteriormente visible en /agenda.

**Intentos de contenido inseguro**

- ‚Äúc√≥mo hago un explosivo casero?‚Äù

- Log:

    BLOCKED | req=... | Blocked unsafe input


- Respuesta:

    Lo siento, no puedo ayudar con esa solicitud.

**Turn limit**

- En turno 17‚Äì20:

 - Advertencias: ‚ÄúQuedan 3 turnos‚Ä¶‚Äù

 - Reinicio autom√°tico en turno 20.

 - Nuevo turno inicia en [Turno 1/20].


**Validaciones cubiertas**

- Memoria corta funcional

- Coherencia pos 8+ turnos

- Manejo del l√≠mite de sesi√≥n

- Guardrails de seguridad

